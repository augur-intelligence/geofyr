{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9cf9117-96ee-48cc-9335-9a01076e007c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 8987.1123046875\n",
      "LOSS: 10395.85546875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-dcf2d0bd8941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobe_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch import nn\n",
    "from torch import cos, sin, arccos, mean, square, sqrt, arcsin\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "import random\n",
    "from datetime import datetime as dt\n",
    "from utils import \n",
    "\n",
    "## MODEL\n",
    "BASE_MODEL = '2021-11-12_model-distilroberta-base_loss-huber_epoch-4'\n",
    "TOKEN_MODEL = 'distilroberta-base'\n",
    "MAX_SEQ_LENGTH = 300\n",
    "NUM_LABELS = 2\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 4\n",
    "LOSS = 'huber'\n",
    "DATE = str(dt.now().date())\n",
    "LOGSTR = f\"{DATE}_model-{TOKEN_MODEL}_loss-{LOSS}\"\n",
    "\n",
    "\n",
    "data = pd.read_parquet('gs://geobert/data/geo_data.parquet')\n",
    "data = data.dropna().drop_duplicates('text')\n",
    "texts = data[\"text\"].str.replace('\\n', '').values.tolist()\n",
    "labels = data[['lat', 'lon']].values.astype(float).tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=.2)\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKEN_MODEL)\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained(TOKEN_MODEL)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "# test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)    \n",
    "\n",
    "train_dataset = GEODataset(train_encodings, train_labels)\n",
    "val_dataset = GEODataset(val_encodings, val_labels)\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "config = RobertaConfig()\n",
    "config.num_labels = NUM_LABELS\n",
    "config.max_position_embeddings = MAX_SEQ_LENGTH\n",
    "\n",
    "model = RobertaForSequenceClassification(config).from_pretrained(BASE_MODEL)\n",
    "# model = DistilBertForSequenceClassification(config).from_pretrained(BASE_MODEL)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = list(DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True))\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs\")\n",
    "losses = []\n",
    "iteration = 0\n",
    "for epoch in list(range(5,15)):\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        logits = outputs.get('logits')\n",
    "        loss_fct = nn.HuberLoss()\n",
    "        train_loss = loss_fct(logits.view(-1, 2), labels.view(-1,2))\n",
    "        train_loss.backward()\n",
    "        optim.step()\n",
    "        train_loss_float = float(train_loss) \n",
    "        del input_ids, attention_mask, labels, logits, train_loss\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_batch = random.choice(val_loader)\n",
    "            val_input_ids = val_batch['input_ids'].to(device)\n",
    "            val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "            val_labels = val_batch['labels'].to(device)\n",
    "            val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels, output_hidden_states=True)\n",
    "            val_logits = val_outputs.get('logits')\n",
    "            val_loss = loss_fct(val_logits, val_labels)\n",
    "            val_loss_float = float(val_loss)\n",
    "            del val_input_ids, val_attention_mask, val_labels, val_logits, val_loss\n",
    "        \n",
    "        losses.append([\n",
    "            iteration, \n",
    "            train_loss_float, \n",
    "            val_loss_float\n",
    "        ])\n",
    "        print(f\"TRAIN: {train_loss_float:.3f}, VAL: {val_loss_float:.3f}\")\n",
    "        writer.add_scalar(LOGSTR + \"-train\", train_loss_float, iteration)\n",
    "        writer.add_scalar(LOGSTR + \"-test\", val_loss_float, iteration)\n",
    "        iteration +=1\n",
    "        \n",
    "    \n",
    "    model.save_pretrained(LOGSTR + f\"_epoch-{epoch}\")\n",
    "\n",
    "model.eval()\n",
    "model.save_pretrained(LOGSTR + f\"_epoch-{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8bd28a10-764a-410e-a721-4bc09a3b93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc2af605-1237-4597-b082-325361ae5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('text-classification', model=model, tokenizer=tokenizer, return_all_scores=True, function_to_apply=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d807471-673e-4c70-bdab-4b7c3c96cdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.7113227248191833},\n",
       "  {'label': 'LABEL_1', 'score': 0.4895717203617096}]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Ich fahre durchs schöne Berlin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
